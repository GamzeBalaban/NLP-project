{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada5815",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "import kagglehub\n",
    "import os\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"nelgiriyewithana/mcdonalds-store-reviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "print(os.listdir(path))\n",
    "\n",
    "\n",
    "file_path= \"/root/.cache/kagglehub/datasets/nelgiriyewithana/mcdonalds-store-reviews/versions/1/McDonald_s_Reviews.csv\"\n",
    "df=pd.read_csv(file_path,encoding=\"ISO-8859-1\")\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8c2a5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df['rating'] = df['rating'].replace({\n",
    "    '1 star': 1, \n",
    "    '2 stars': 2, \n",
    "    '3 stars': 3, \n",
    "    '4 stars': 4, \n",
    "    '5 stars': 5\n",
    "}).astype(int)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99813dd5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "df.groupby('rating').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
    "plt.gca().spines[['top', 'right',]].set_visible(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08686933",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(str(df['cleaned_review']))\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3618f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "def term_frequency(text):\n",
    "    \"\"\"Calculates the term frequency for a given text.\"\"\"\n",
    "    words = text.split()\n",
    "    word_counts = Counter(words)\n",
    "    total_words = len(words)\n",
    "    # Use a different variable name to avoid conflict with external 'tf'\n",
    "    term_freq = {word: count / total_words for word, count in word_counts.items()}\n",
    "    return term_freq\n",
    "\n",
    "\n",
    "df['term_frequency'] = df['cleaned_review'].apply(term_frequency)\n",
    "\n",
    "print(df[['cleaned_review', 'term_frequency']].head())\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_words = []\n",
    "for tf in df['term_frequency']:\n",
    "    all_words.extend(list(tf.keys()))\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "top_words_all = dict(sorted(word_counts.items(), key=lambda item: item[1], reverse=True)[:10])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_words_all.keys(), top_words_all.values())\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Total Frequency\")\n",
    "plt.title(\"Top 10 Words Across All Reviews\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b6d339",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import kagglehub\n",
    "import os\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')  \n",
    "nltk.download('wordnet')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "path = kagglehub.dataset_download(\"nelgiriyewithana/mcdonalds-store-reviews\")\n",
    "file_path = os.path.join(path, \"McDonald_s_Reviews.csv\")\n",
    "df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
    "\n",
    "#boş yorumlar  ve eksik veriler silinir.\n",
    "df['rating'] = df['rating'].astype(str).str.extract(r'(\\d+)').astype(int)\n",
    "df.dropna(subset=['review'], inplace=True)\n",
    "df = df[df['review'].str.strip() != \"\"]\n",
    "\n",
    "# Lemmatizer ve stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = contractions.fix(text) #kısa kelimeleri açar don't->do not\n",
    "    text = text.lower()\n",
    "    # özel karakterleri temizler\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)   \n",
    "    # birden fazla boşluğu tek bir boşlukla değiştirip baştaki ve sondaki boşlukları kaldırır\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    words = word_tokenize(text) #cümleyi kelimelere ayırır\n",
    "    # Stopwords ve noktalama işaretlerini çıkarır ve kelimeleri lemmatize eder\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and word.isalpha()]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df['review'] = df['review'].apply(clean_text)\n",
    "#etiketleme\n",
    "df['label'] = df['rating'].map({1: 0, 2: 0, 3: 1, 4: 2, 5: 2})\n",
    "\n",
    "df.to_csv(\"cleaned_review.csv\", index=False)\n",
    "\n",
    "#VERİ TOKENİZASYONU#\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encode_texts(texts, tokenizer, max_length=256): #metinleri BERT'in anlayacağı formata çevirir\n",
    "    encodings = tokenizer.batch_encode_plus(\n",
    "        texts.tolist(), #listeye çevirir\n",
    "        max_length=max_length,\n",
    "        truncation=True, #max aşılırsa keser\n",
    "        padding='max_length', #doldurma \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encodings #input_ids ve attention_mask döndürülür\n",
    "\n",
    "encodings = encode_texts(df['review'], tokenizer)\n",
    "labels = torch.tensor(df['label'].values) \n",
    "\n",
    "#bert için gerekli 3 bileşeni oluşturur\n",
    "dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], labels)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "#veri batch olarak modele aktarılır . parça para \n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Model oluşturma\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, num_labels=3):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.bert(input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "model = SentimentModel().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5) #model ağırlıkları optimize edklir\n",
    "criterion = nn.CrossEntropyLoss()#kayıp fonk\n",
    "\n",
    "# Model eğitimi\n",
    "#her batch için loss ve accuarcu hesaplar \n",
    "def train(model, train_loader, val_loader, optimizer, criterion, epochs=3):\n",
    "    for epoch in range(epochs):\n",
    "        model.train() #model eğitim modunda (katmanların yanlış çalışmasını engeller)\n",
    "        total_loss, total_correct, train_preds, train_labels = 0, 0, [], []\n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)#crossentropy\n",
    "            loss.backward() #kayıp fonk elen gradyanları hesaplar\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "            # Eğitimdeki tahminleri toplama\n",
    "            train_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Eğitim doğruluğu ve F1 skoru hesaplama\n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        train_f1 = f1_score(train_labels, train_preds, average='weighted')  \n",
    "\n",
    "        # Doğrulama doğruluğu ve F1 skoru hesaplama\n",
    "        val_acc, val_f1 = evaluate(model, val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Loss: {total_loss / len(train_loader):.4f}, \"\n",
    "              f\"Train Accuracy: {train_acc:.4f}, Train F1: {train_f1:.4f}, \"\n",
    "              f\"Val Accuracy: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "# Modeli değerlendirme\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    true_vals, predictions = [], [] #gerçek etiket ve model tahmini\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "            logits = model(input_ids, attention_mask)#model çıktılarını hesaplar\n",
    "            preds = logits.argmax(dim=1).cpu().numpy() #model tahminleri alınır\n",
    "            predictions.extend(preds)\n",
    "            true_vals.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Accuracy ve F1 skorunu hesapla\n",
    "    val_acc = accuracy_score(true_vals, predictions)\n",
    "    val_f1 = f1_score(true_vals, predictions, average='weighted')\n",
    "    return val_acc, val_f1\n",
    "\n",
    "# Modeli eğit\n",
    "train(model, train_loader, val_loader, optimizer, criterion, epochs=3)\n",
    "\n",
    "# Modeli kaydet ağırlıklarıyla \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "torch.save(model.state_dict(), '/content/drive/My Drive/bert_model2.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec19cf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Epoch 1: Loss: 0.5212, Train Accuracy: 0.7971, Train F1: 0.7749, Val Accuracy: 0.8251, Val F1: 0.8155\n",
    "Epoch 2: Loss: 0.4016, Train Accuracy: 0.8444, Train F1: 0.8348, Val Accuracy: 0.8329, Val F1: 0.8230\n",
    "Epoch 3: Loss: 0.3011, Train Accuracy: 0.8856, Train F1: 0.8819, Val Accuracy: 0.8371, Val F1: 0.8316\n",
    "Mounted at /content/drive"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
